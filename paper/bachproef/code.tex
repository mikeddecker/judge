\chapter{\IfLanguageName{dutch}{Stukken code}{Code snippets}}%
\label{ch:code-snippets}


In this appendix, code snippets to prevent code saturation in between text.

(Code example will be deleted in the final submit)

\begin{listing}
    \begin{minted}{python}
        import pandas as pd
    \end{minted}
    \caption[]{}
    \label{code:}
\end{listing}


\section{Localization}
\label{sec:code-localization}

\begin{listing}
    \begin{minted}{python}
        DefaultMaxPool = functools.partial(
            keras.layers.MaxPool2D,
            pool_size=(3,3), strides=(2,2), padding="same")

        def get_googlenetsmall_model(input_shape, num_classes, use_batch_norm=True, **kwargs):
            model = keras.Sequential(**kwargs)
            model.add(keras.layers.Input(shape=input_shape))
            model.add(DefaultConv(filters=24, kernel_size=(7,7), strides=(2,2),  padding='same'))
            if use_batch_norm:
                model.add(keras.layers.BatchNormalization())
            model.add(DefaultMaxPool())
            model.add(DefaultConv(filters=32))
            model.add(DefaultConv(filters=48, kernel_size=(3,3)))
            if use_batch_norm:
                model.add(keras.layers.BatchNormalization())
            model.add(DefaultMaxPool())

            model.add(InceptionModule(filters11=32, filters33_reduce=48, filters33=64,
                filters55_reduce=8, filters55=16, filters_pool_proj=16,
                use_batch_norm=use_batch_norm))
            model.add(InceptionModule(filters11=64, filters33_reduce=64, filters33=96,
                filters55_reduce=16, filters55=48, filters_pool_proj=32,
                use_batch_norm=use_batch_norm))
            model.add(DefaultMaxPool())

            # ... see part 2
    \end{minted}
    \caption[GoogleNet implementation part 1]{GoogleNet implementation part 1}
    \label{code:keras-googlenet-small-replication}
\end{listing}

\begin{listing}
    \begin{minted}{python}
        def get_googlenetsmall_model(input_shape, num_classes, use_batch_norm=True, **kwargs):
            # ... see part 1

            model.add(InceptionModule(filters11=96, filters33_reduce=48, filters33=104,
                filters55_reduce=8, filters55=24, filters_pool_proj=32,
                use_batch_norm=use_batch_norm))
            model.add(InceptionModule(filters11=80, filters33_reduce=56, filters33=224,
                filters55_reduce=12, filters55=32, filters_pool_proj=32,
                use_batch_norm=use_batch_norm))
            model.add(InceptionModule(filters11=64, filters33_reduce=64, filters33=256,
                filters55_reduce=12, filters55=32, filters_pool_proj=32,
                use_batch_norm=use_batch_norm))
            model.add(InceptionModule(filters11=56, filters33_reduce=64, filters33=144,
                filters55_reduce=16, filters55=32, filters_pool_proj=32,
                use_batch_norm=use_batch_norm))
            model.add(InceptionModule(filters11=128, filters33_reduce=80, filters33=160,
                filters55_reduce=16, filters55=64, filters_pool_proj=64,
                use_batch_norm=use_batch_norm))

            model.add(DefaultMaxPool())
            model.add(InceptionModule(filters11=128, filters33_reduce=80, filters33=160,
                filters55_reduce=16, filters55=64, filters_pool_proj=64,
                use_batch_norm=use_batch_norm))
            model.add(InceptionModule(filters11=192, filters33_reduce=92, filters33=192,
                filters55_reduce=24, filters55=64, filters_pool_proj=64,
                use_batch_norm=use_batch_norm))
            model.add(keras.layers.GlobalAveragePooling2D())
            model.add(keras.layers.Dropout(0.4))
            model.add(keras.layers.Flatten())
            model.add(keras.layers.Dense(units=256, activation="relu"))
            model.add(keras.layers.Dense(units=num_classes, activation='sigmoid'))

            return model
    \end{minted}
    \caption[GoogleNet implementation part 1]{GoogleNet implementation part 2}
    \label{code:keras-googlenet-small-replication}
\end{listing}


\begin{listing}
    \begin{minted}{python}
        import functools
        import keras

        DefaultConv = functools.partial(
            keras.layers.Conv2D, kernel_size=(3, 3), strides=(2, 2),
            padding="same", activation="relu", kernel_initializer="he_normal")

        DefaultMaxPool = functools.partial(
            keras.layers.MaxPool2D,
            pool_size=(3,3), strides=(2,2), padding="same")

        def get_model(modelinfo: dict, **kwargs):
            dim = modelinfo['dim']
            model = keras.Sequential(**kwargs)
            mobilenetv3small = keras.applications.MobileNetV3Small(
                input_shape=(dim,dim,3),
                include_top=False,
                weights="imagenet",
                dropout_rate=0.2,
                pooling='avg',
                name="MobileNetV3Small",
            )
            mobilenetv3small.trainable = False
            model.add(mobilenetv3small)
            model.add(keras.layers.Dense(units=512, activation="relu"))
            model.add(keras.layers.Dense(units=128, activation="relu"))
            model.add(keras.layers.Dense(units=4, activation='sigmoid'))

            return model
    \end{minted}
    \caption[keras mobilenet full boxes]{Usage of mobilenet for full box predictions}
    \label{code:keras-mobilenetv3small}
\end{listing}

\begin{listing}
    \begin{minted}{python}
        # To long to properly include in the paper
        # https://github.com/mikeddecker/judge/blob/main/computervision/localizor_with_strats.py
    \end{minted}
    \caption[Localizor with strats]{Localizor with strats}
    \label{code:localizor-with-strats}
\end{listing}

\section{Segmentation}
\label{sec:code-segmentation}

\begin{listing}
    \begin{minted}{python}
        def calculate_splitpoint_values(videoId: int, frameLength:int, df_Skills:pd.DataFrame, fps:float, Nsec_frames_around=1/6):
        """Creates a dataframe: 'videoId', 'frameNr', 'splitpoint'
        Where splitpoint is the value 0 -> 1 whether the video needs to be split at that point or not"""
        splitpoint_values = {
            'videoId' : [videoId for _ in range(frameLength)],
            'frameNr' : range(frameLength),
            'splitpoint' : [0 for _ in range(frameLength)],
        }

        frames_around_splitpoint = round(Nsec_frames_around * fps)
        for _, skillrow in df_Skills.iterrows():
            frameStart = skillrow["frameStart"]
            frameEnd = skillrow["frameEnd"]

            currentFrameStart = frameStart - frames_around_splitpoint
            currentFrameEnd = frameEnd - frames_around_splitpoint
            while currentFrameStart < frameStart + frames_around_splitpoint:
                framesApart = abs(currentFrameStart - frameStart)
                splitvalue = 1 - (framesApart/frames_around_splitpoint) ** 2
                splitvalue *= splitvalue

                currentFrameStart += 1
                currentFrameEnd += 1

                splitpoint_values['splitpoint'][currentFrameStart] = splitvalue
                if currentFrameEnd < frameLength:
                    splitpoint_values['splitpoint'][currentFrameEnd] = splitvalue

        return pd.DataFrame(splitpoint_values)
    \end{minted}
    \caption[call-splitpoint-calculation]{call-splitpoint-calculation}
    \label{code:calculate-splitpoint-values}
\end{listing}


\begin{listing}
    \begin{minted}{python}
        df = calculate_splitpoint_values(
            videoId=videoId,
            frameLength=frameLength,
            df_Skills=self.Skills[self.Skills['videoId'] == videoId],
            fps = row["fps"]
        )
    \end{minted}
    \caption[call-splitpoint-calculation]{call-splitpoint-calculation}
    \label{code:call-splitpoint-calculation}
\end{listing}


\section{Recognition}
\label{sec:code-recognition}

\begin{listing}
    \begin{minted}{python}
        # ConfigHelper.py
        def get_discipline_DoubleDutch_config(include_tablename=True):
            config = {
                "Type" : ("Categorical", "Type"), # Will be textual representions
                "Rotations" : ("Numerical", 0, 8, 1), # min, max, step
                "Turner1": ("Categorical", "Turner"),
                "Turner2": ("Categorical", "Turner"),
                "Skill" : ("Categorical", "Skill"),
                "Hands" : ("Numerical", 0, 2, 1), # 0 for al salto types
                "Feet" : ("Numerical", 0, 2, 1),
                "Turntable" : ("Numerical", 0, 4, 0.25), # Per quarter (but still integers)
                "BodyRotations" : ("Numerical", 0, 2, 0.5),
                "Backwards" : ("Boolean"),
                "Sloppy" : ("Boolean"),
                "Hard2see" : ("Boolean"),
                "Fault" : ("Boolean"),
            }
            if include_tablename:
                config["Tablename"] = "DoubleDutch"
            return config
    \end{minted}
    \caption[confighelper skill configuration]{ConfigHelper skill configuration for labeling aspects of a skill, devided in Categorical, Numerical and Boolean output values of skills. All values of the numerical output are integers, which is why you need to multiply by the step in order to get the actual numerical representation.}
    \label{code:confighelper}
\end{listing}

\begin{listing}
    \begin{minted}{python}
        import keras
        import pandas as pd
        import tensorflow as tf
        import numpy as np
        import matplotlib.pyplot as plt
        import sys
        sys.path.append('..')
        from api.helpers import ConfigHelper
    \end{minted}
    \caption[imports-ViViT]{imports-ViViT}
    \label{code:imports-ViViT}
\end{listing}


\begin{listing}
    \begin{minted}{python}
        def mlp(x, hidden_units, dropout_rate):
            for units in hidden_units:
                x = keras.layers.Dense(units, activation=keras.activations.gelu)(x)
                x = keras.layers.Dropout(dropout_rate)(x)
            return x
    \end{minted}
    \caption[mlp layer]{ViViT: multi-layer-perceptron}
    \label{code:mlp-layer}
\end{listing}


\begin{listing}
    \begin{minted}{python}
    def __init__(self, patch_size):
        super().__init__()
        self.patch_size = patch_size

    def call(self, images):
        input_shape = keras.ops.shape(images)
        batch_size = input_shape[0]
        timestep = input_shape[1]
        height = input_shape[2]
        width = input_shape[3]
        channels = input_shape[4]
        num_patches_h = height // self.patch_size
        num_patches_w = width // self.patch_size

        def create_single_timepatch(video_input):
            patches = keras.ops.image.extract_patches(video_input, size=self.patch_size)
            patches = keras.ops.reshape(
                patches,
                (
                    num_patches_h * num_patches_w * timestep,
                    self.patch_size * self.patch_size * channels,
                ),
            )

            return patches

        patches = tf.map_fn(create_single_timepatch, images)

        return patches

    \end{minted}
    \caption[time-patches]{ViViT: Time Patches}
    \label{code:timepatches}
\end{listing}


\begin{listing}
    \begin{minted}{python}
        def get_model(modelinfo):
            inputs = keras.Input(shape = (modelinfo['timesteps'], modelinfo['dim'], modelinfo['dim'], 3))
            patches = TimePatches(modelinfo['patch_size'])(inputs)
            num_patches = (modelinfo['dim'] // modelinfo['patch_size']) ** 2
            encoded_patches = TimePatchEncoder(num_patches, modelinfo['timesteps'], modelinfo['dim_embedding'])(patches)
            print("shape of encoded_patches", encoded_patches.shape)

            # Create multiple layers of the Transformer block.
            for _ in range(modelinfo['encoder_blocks']):
                # Layer normalization 1.
                x1 = keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
                attention_output = keras.layers.MultiHeadAttention(
                num_heads=modelinfo['num_heads'], key_dim=modelinfo['dim_embedding'], dropout=0.1
                )(x1, x1)
                # Skip connection 1.
                x2 = keras.layers.Add()([attention_output, encoded_patches])
                # Layer normalization 2.
                x3 = keras.layers.LayerNormalization(epsilon=1e-6)(x2)
                x3 = mlp(x3, hidden_units=[modelinfo['dim_embedding'] ** 2, modelinfo['dim_embedding']], dropout_rate=0.1)
                # Skip connection 2.
                encoded_patches = keras.layers.Add()([x3, x2])

            # Create a [batch_size, projection_dim] tensor.
            representation = keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
            representation = keras.layers.Flatten()(representation)
            representation = keras.layers.Dropout(0.3)(representation)

            features = mlp(representation, hidden_units=modelinfo['mlp_head_units'], dropout_rate=0.3)

            ...

    \end{minted}
    \caption[get model ViViT]{ViViT: get model ViViT (keras), without output layer (part 1)
    (Possible bugfixes done using \textcite{OpenAI_ChatGPT_2025})}
    \label{code:get_model_ViViT}
\end{listing}

\begin{listing}
    \begin{minted}{python}
        # ... (this is the output layer of segmentation)
        classes = modelinfo['timesteps']
        outputs = keras.layers.Dense(classes, activation='softmax')(features)

        return keras.Model(inputs=inputs, outputs=outputs)
    \end{minted}
    \caption[ViViT output segmentation]{ViViT output segmentation using features of code fragment \ref{code:get_model_ViViT}}
    \label{code:ViViT-output-segmentation}
\end{listing}


\begin{listing}
    \begin{minted}{python}
        predictions = np.array(predictions)
        predictions_bigger_than_split_threshold = np.where(predictions > split_threshold, predictions, 0)
        p_split = predictions_bigger_than_split_threshold
        window_size = int(fps // 3)
        predictions_argMax_in_window = [s - window_size + np.argmax(p_split[max(0, s-window_size):min(frameLength, s+window_size)]) for s in range(frameLength)]
        predictions_splitmoments = np.where(predictions > split_threshold, predictions_argMax_in_window, 0)
        predictions_splitmoments = np.unique(predictions_splitmoments)

        distances = predictions_splitmoments[1:] - predictions_splitmoments[:-1]
        predictions_splitmoments = predictions_splitmoments[1:]
        predictions_splitmoments = predictions_splitmoments[np.where(distances < window_size // 3, False, True)]
        predictions_splitmoments = [int(g) for g in predictions_splitmoments]
    \end{minted}
    \caption[predictions-to-splitpoints]{Code which filters splitpoints from the raw predicted splitpoint values to frame numbers.}
    \label{code:predictions-to-splitpoints}
\end{listing}


\begin{listing}
    \begin{minted}{python}
        dd_config = ConfigHelper.get_discipline_DoubleDutch_config()
        outputs = {}
        for key, value in dd_config.items():
            if key == "Tablename":
                continue
            if value[0] == "Categorical":
                tablename = "skill"
                match (key):
                    case 'Skill':
                        tablename = 'skills'
                    case 'Turner1' | 'Turner2':
                        tablename = "turners"
                    case 'Type':
                        tablename = 'types'
                classes = int(df_table_counts.iloc[0][tablename] + 1) # Because of indexing
                outputs[key] = keras.layers.Dense(classes, activation='softmax', name=key)(features)
            else:
                outputs[key] = keras.layers.Dense(1, activation='sigmoid', name=key)(features)

        return keras.Model(inputs=inputs, outputs=outputs)
    \end{minted}
    \caption[Keras output layers for skills]{Keras output layers for skills}
    \label{code:keras-skill-top-layers}
\end{listing}

\begin{listing}
    \begin{minted}{python}
        def create_pytorch_skill_output_layers(lastNNeurons, balancedType, df_table_counts):
            dd_config = get_discipline_DoubleDutch_config()
            output_layers = torch.nn.ModuleDict()
            
            for key, value in dd_config.items():
                if key == "Tablename":
                    continue
                if value[0] == "Categorical":
                    columnname = "skill"
                    if key == 'Skill':
                        columnname = 'skills'
                    elif key in ['Turner1', 'Turner2']:
                        columnname = "turners"
                    elif key == 'Type':
                        columnname = 'types'
                    
                    classes = int(df_table_counts.iloc[0][columnname] + 1) # Because of MysqlIndex starts from 1
                    output_layers[key] = torch.nn.Linear(lastNNeurons, classes)
                else:
                    output_layers[key] = torch.nn.Linear(lastNNeurons, 1)
            
            if balancedType == 'jump_return_push_frog_other':
                output_layers['Skill'] = torch.nn.Linear(lastNNeurons, 5)
            
            return output_layers
    \end{minted}
    \caption[Pytorch skill output layers]{PyTorch skill output layers, uses \ref{code:confighelper}}
    \label{code:pytorch-skill-output-layers}
\end{listing}

\begin{listing}
    \begin{minted}{python}
        class MViT(nn.Module):
            def __init__(self, skill_or_segment:str, modelinfo:dict, df_table_counts:pd.DataFrame):
                super(MViT, self).__init__()
                self.modelinfo = modelinfo
                self.df_table_counts = df_table_counts
                self.isSkillModel = skill_or_segment == "skills"
                
                input_shape = (3, modelinfo['timesteps'], modelinfo['dim'], modelinfo['dim'])
                self.mvit = models.video.mvit_v1_b(weights='DEFAULT')
                self.mvit = self.mvit.to('cuda').eval()

                for param in self.mvit.parameters():
                    param.requires_grad = False

                self.mvit.head = torch.nn.Identity()  # This removes the top layer
                self.flatten = nn.Flatten()
                self.LastNNeurons = self._get_mvit_output(input_shape)
                
                if self.isSkillModel:
                    self.output_layers = create_pytorch_skill_output_layers(lastNNeurons=self.LastNNeurons, balancedType=modelinfo['balancedType'], df_table_counts = self.df_table_counts)
                else:
                    self.output_layer = create_pytorch_segmentation_output_layers(lastNNeurons=self.LastNNeurons, timesteps=modelinfo['timesteps'])

                
            def _get_mvit_output(self, shape):
                with torch.no_grad():
                    input = torch.rand(1, *shape).to('cuda')
                    output = self.mvit(input)
                    output = self.flatten(output)
                    return output.shape[1]
    \end{minted}
    \caption[Pytorch MViT implementation init]{Pytorch MViT implementation init, uses \ref{code:pytorch-skill-output-layers} and \ref{code:skill-forward}}
    \label{code:pytorch-mvit-init}
\end{listing}

\begin{listing}
    \begin{minted}{python}
    def forward(self, x):
        # Input shape: (batch_size, channels, timesteps, height, width)
        x = self.mvit(x)
        x = self.flatten(x)
        
        if self.isSkillModel:
            return forward_skill_output_layers(features=x, output_layers=self.output_layers)
        else:
            return forward_segmentation_output_layers(features=x, output_layer=self.output_layer)
    \end{minted}
    \caption[Pytorch MViT forward]{Pytorch MViT implementation of forward method, used by \ref{code:pytorch-mvit-init}}
    \label{code:pytorch-forward}
\end{listing}



\begin{listing}
    \begin{minted}{python}
        def forward_skill_output_layers(features: torch.tensor, output_layers: dict[str, torch.nn.Module]):
            outputs = {}
            for key, layer in output_layers.items():
                if key in ['Skill', 'Turner1', 'Turner2', 'Type']:
                    outputs[key] = layer(features)
                else:  # Regression outputs
                    outputs[key] = torch.sigmoid(layer(features))
            
            return outputs
    \end{minted}
    \caption[Pytorch skill forward feed]{PyTorch skill forward feeding}
    \label{code:pytorch-skill-layers-forward}
\end{listing}

\begin{listing}
    \begin{minted}{python}
        # Adapting the losses, as limiting to 10% can change occurences of faults, bodyrotations... a little
        for key, value in config.items():
            value_counts_train = train_generator.BalancedSet[ConfigHelper.lowerProperty(key)].value_counts(dropna=False)
            value_counts_val = val_generator.Skills[ConfigHelper.lowerProperty(key)].value_counts(dropna=False)
            value_counts_combined = value_counts_train.add(value_counts_val, fill_value=0)
        
            maximum = value_counts_combined.max()
        
            weights = (maximum + maximum // 8 - value_counts_combined).pow(0.75)
            weights = weights / weights.mean()
            if value[0] == 'Categorical':
                weights.loc[0] = 0
            weights = weights.sort_index()
        
            w_all = torch.ones(value_counts_combined.index.max() + 1, dtype=torch.float32).to(device=device)
            for idx, w in weights.items():
                w_all[idx] = w
            w_all = (w_all + 1) ** 2
        
            print("loss weights for", key, w_all)
            if value[0] == 'Categorical':
                loss_fns[key] = torch.nn.CrossEntropyLoss(w_all).to(device=device)
            else:
                loss_fns[key] = lambda input, target: weighted_mse_loss(input=input, target=target, weight=w_all)
    \end{minted}
    \caption[Code calculating weights for the loss functions]{Code calculating weights for the loss functions}
    \label{code:recognition-weighted-loss}
\end{listing}

\begin{listing}
    \begin{minted}{python}
        def weighted_mse_loss(input, target, weight):
            "https://discuss.pytorch.org/t/how-to-implement-weighted-mean-square-error/2547"
            return torch.sum(weight * (input - target) ** 2)
    \end{minted}
    \caption[Weihted MSE]{Weighted MSE}
    \label{code:weighted-mse}
\end{listing}

