%%=============================================================================
%% Results
%%=============================================================================

\chapter{\IfLanguageName{dutch}{Resultaten}{Results}}%
\label{ch:results}

The result section focuses on elaborating conducted experiments and acquired results, divided into the three main phases, localization, segmentation and recognition. In order to label, annotate videos and gain insights in the results a Vue3 web-app is created on a python flask API which reads and stores labels and video annotations from a MySQL database. Furthermore, the experiments are all performed using an Acer Nitro ANV15-51, running Ubuntu 24.04.2 LTS, using a 13th Gen Intel® Core™ i5-13420H × 12, with 16GB RAM and a NVIDIA GeForce RTX™ 4050 Laptop GPU 5898MiB.

\section{Jumper localization}

% TODO: add, this is relevant
% TODO: convex hull the 3 jumpers and train on yolo model.
As DD3 routines are the focus of this research, the idea was to predict the coordinates and position of a team as a single unit, instead of individuals. This idea was supported by the fact that there aren't any public jump rope datasets and it was expected to increase the speed of labeling. The AI generated image \ref{fig:sr2-performance-ai-generated} shows an example of a competition setting where two athletes are performing a routine. The goal here would be to crop out the jumpers.

% TODO : add crop around jupers

Below you can find a shortlist of experiments executed on localization.

\begin{itemize}
    \item random conv
    \item googlenet, mobilenet...
    \item Mask-RCNN
    \item Labeling individual jumpers \& pre-trained model yolo -> OK
\end{itemize}

\subsection{Random convolutional network}

% TODO : illustrative image of box coords?

Before discussing the different results, let's discuss how the location of skippers are marked. To mark the position of an object on an image, there are multiple possibilities.
For this project, the relative center point along the x-axis, y-axis, width and height of the box are stored. So regardless of the scale of the image, whether the image has size 1920 x 1080 pixels or 1080 x 720 pixels, the position of the box remains the same.
An example of a box would be [0.6, 0.5, 0.4, 0.4], all values between 0 and 1. IoU accuracy can then be performed.
... As explained in the literature (todo reference to lit)...
% TODO : shift IoU comparison to literature

In order to get an idea, whether following models are able to learn, a random convolutional network is created.
... elaborate results % TODO

-> low accuracy, naive predictions.

To get a first idea whether it would start learning, so a transition can be made towards more advanced architectures.

\subsection{Dedicated architectures}

From scratch

GoogleNet, MobileNet... (on individual labels)

Display code \& mention unbalanced augmented, but still unbalanced dataset \& higher learning rate as compared below.

Conclusion:
Predicting full teams didn't work out which allowed for a transition to labeling and predicting individuals athletes. Below you find more information about those experiments.

\subsection{Mask-RCNN}
Failed experiment, C error, no results.

\subsection{YOLO}

Experiment 3, pre-trained models \& individual boxes.

The third experiment incorporates a double upgrade. The first upgrade involves annotating individual athletes on images instead of one big box around all jumpers. This method allows to annotate other routines such as single rope, chinese wheel or double dutch 4. The second upgrade is trying a pre-trained object detection model.

Ultralytics \autocite{Khanam2024} provides an easy-to-use pre-trained implementation for predicting people and objects in images which can be fine-tuned for specific use cases. Fine-tuning was needed as spectators, also humans, were also included in the predictions.

Using these predictions, a crop around the three jumpers can be created using the predicted jumper boxes from the fine-tuned YOLO model, after using the pre-trained weights on the COCO dataset \autocite{Lin2014}. In order to improve the crops some steps were required. Details are discussed below.

\subsubsection{Crop stability}

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{img/1267_292_boxes}
    \includegraphics[width=0.95\linewidth]{img/1267_292_boxes_reduced_spectators}
    \caption[raw vs fine-tuned YOLOv11 nano model predictions]{Raw predictions of the unrefined YOLOv11 nano model compared to the fine-tuned model which reduces predictions of spectators.}
    \label{fig:raw-vs-fine-tuned-boxes}
\end{figure}

\begin{figure}
    \centering
%%    \includegraphics[width=0.35\linewidth]{img/PR_curve}
    \includegraphics[width=0.95\linewidth]{img/results}
    \includegraphics[width=0.85\linewidth]{img/confusion_matrix_normalized}
    \caption[metrics after fine-tuning YOLOv11]{Metrics after fine-tuning YOLOv11 on the validation set of 161 images, mAP50 of 0.953 \& mAP50-95 of 0.63, annotation of coaches was an idea.}
    \label{fig:localization-results}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.45\linewidth]{img/1315_2935}
    \includegraphics[width=0.45\linewidth]{img/2297_134}
    \caption[Valid crops of a DD3 routine on competition.]{Valid crops of a DD3 routine on competition.}
    \label{fig:dd3-crop}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.45\linewidth]{img/1267_292_cropped}
    \includegraphics[width=0.45\linewidth]{img/1405_1061_cropped}
    \caption[dd3-crop-error]{Two crops were a spectator alongside the athletes.}
    \label{fig:dd3-crop-error}
\end{figure}

Annotating only athletes on 732 images \& fine-tuning the pre-trained nano yolo model, the first predictions of spectators are reduced considerately.
This way crops can be created by taking the minimum and maximum \(x\) \& \(y\)-coordinates. Reviewing cropped videos show indicate occasional frames predicting spectators, coaches or judges and moments of instability, as if you are watching camera footage of an earthquake.

In order to avoid a sudden zoom out, because of spectators, an IoU comparison of the previous predicted box with the new box is made. If the IoU value is larger than the average IoU of the last \(N\) seconds, powered to the fourth, than crop coordinates will be updated. Using the IoU comparison, reduces video crops including spectators, maintaining the earthquake effect.


To reduce shakiness and maintain a nice to watch cropped video, box-coordinates are smoothed out adapting box predictions of previous frames with the current one. This way, drastic changes in a predicted location, e.g. model ererrorror, arm or field movements aren't that drastic. Adaptation is done by incorporating two smooth values, indicating how much weight you give to the smoothed prediction of the previous frame. \footnote{Smoothed values are obtained by playing around with values, choosing those leading to the best, subjective, smoothed review}

$$ smth = smootval = 0.86  \quad \& \quad smths = smootval\_shrink = 0.94 $$

Which makes the new coordinates:

\begin{math}
   smooted_{x1_{min}} =
   \begin{cases}
       smooted_{x1_{min}} & \text{if} \quad iou < iou_{threshold} \\
       smth * smooted_{x1_{min}} + (1-smth) * x1_{min} & \text{else if} \quad x1_{min} < smooted_{x1_{min}} \\
       smths^ * smooted_{x1_{min}} + (1-smths) * x1_{min} & \text{otherwise}
   \end{cases}
\end{math}

The same is done for \( y1_{min}, x2_{max}, y2_{max} \).

Utilizing these steps allows for videos where a team is always on display (\ref{fig:dd3-crop}), with some videos still predicting spectators (\ref{fig:dd3-crop-error}) and a newly introduced problem, namely skippers running out of view.

% TODO : add movement corrector

\begin{figure}
    \centering
    \includegraphics[width=0.45\linewidth]{img/1241_1093_cropped}
    \caption[dd3-crop-error]{Athletes running out of view.}
    \label{fig:dd3-crop-error-out-of-view}
\end{figure}

One way to solve jumpers running out of view, is by adding a default movement corrector, like the smooth value, which follows the new coordinates every frame. The movement corrector is based on the \(iou\)-value powered to the \(1/8\). It makes the crop following predictions by default, when IoU is getting low.


\begin{math}
    SQRT = 8 \\
    movementCorrector = iou^{1/SQRT} \\
    smooted_{x1_{min}} = smooted_{x1_{min}} * movementCorrector + (1 - movementCorrector) * {x1_{min}}
\end{math}



A possible improvement could be matching individual boxes with the new predictions, eliminating the spectator. Which raises concerns about jumpers entering the competition field. There are no earlier predicted boxes to match.
The total algorithm isn't perfect yet, but the results indicate sufficient valid video crops to use and experiment on segmentation and recognition.

\begin{listing}
    \begin{minted}{python}
        print('TODO : cleanup code')
    \end{minted}
    \caption[Example codefragment]{Example of adding cropping code.}
    \label{code:localization}
\end{listing}

Full code is visible in code snippet \ref{code:localization}. The results are broken down in table \ref{tbl:crop-results}.

% TODO: pairwise distances?

\begin{table}[h!]
    \begin{tabular}{|l|l|l|}
        \hline
        \textbf{videoId} & \textbf{seconds} & \textbf{reason} \\
        \hline
        \dots & \cellcolor{green!25} \dots & (other videos) \\ \hline
        1435 &	\cellcolor{green!25} 0 &	 \\ \hline
        1445 &	\cellcolor{green!25} 0 &	 \\ \hline
        2285 &	\cellcolor{green!25} 0 &	 \\ \hline
        2295 &	\cellcolor{green!25} 0 &	 \\ \hline
        2305 &	\cellcolor{green!25} 0 &	 \\ \hline
        2315 &	\cellcolor{green!25} 0 &	 \\ \hline
        995  &	\cellcolor{yellow!25} 5 &	Schocks \& turner out-of-view \\ \hline
        1275 &	\cellcolor{yellow!25} 1 &	aerial half \\ \hline
        1405 &	\cellcolor{orange!25} 30 &	spectator \\ \hline
        651  &	\cellcolor{yellow!25} 1    &	turner out of view \\ \hline
        664  &	\cellcolor{yellow!25} 1.5  &	turner out of view \\ \hline
        1202 &	\cellcolor{yellow!25} 5    &	turner out of view, recording itself \\ \hline
        1178 &	\cellcolor{yellow!25} 2    &	turner out of view \\ \hline
        1238 &	\cellcolor{yellow!25} 2    &	spectator before start routine \\ \hline
        1244 &	\cellcolor{yellow!25} 0    &	stopped following, remained in view \\ \hline
        1257 &	\cellcolor{yellow!25} 0    &	stopped following, remained in view \\ \hline
        1275 &	\cellcolor{yellow!25} 1.5  &	half out of view \\ \hline
        1339 &	\cellcolor{yellow!25} 8    &  spectator \\ \hline
        1444 &	\cellcolor{yellow!25} 1.5  &	half out of view \\ \hline
        2282 &	\cellcolor{yellow!25} 1    &	1 turner out of view, actual 3 sec no follow \\ \hline
        2317 &	\cellcolor{yellow!25} 2    &	minor shocks \\ \hline
        663  &	\cellcolor{orange!25} 25   &	Out-of-view \& shocks \\ \hline
        1241 &	\cellcolor{orange!25} 8    &	Out-of-view \\ \hline
        1267 &	\cellcolor{orange!25} 46   &	spectator \\ \hline
        1271 &	\cellcolor{orange!25} 24   &	Out-of-view \\ \hline
        1273 &	\cellcolor{orange!25} 8    &	Out-of-view \\ \hline
        1326 &	\cellcolor{orange!25} 6.5  &	spectator \\ \hline
        \(avg\) &	1.33 &	(seconds / 1m15) \\ \hline
        \(avg_{if}\) &	8.55 &	(seconds / 1m15) \\ \hline
        Nr of videos not ok &	21	& 15.6\% \\ \hline
        Total checked videos &	135	& (includes train videos) \\ \hline
    \end{tabular}
    \caption{Crop results manually checked. \\
    Yellow indicated videos are not disturbing for recognition, even though the crops aren't perfect. Orange videos require more attention and shouldn't be used for recognition. \\
    The \(avg_{if}\) indicates the average time in case a video has crops which aren't good.}
    \label{tbl:crop-results}
\end{table}

\section{Action segmentation}

\section{Skill recognition}

The first trials from scratch, a video vision transformer, which is adapted from the transformer created during classes. Adding the convolution layer in front and the time dimension created the ViViT transformer as proposed in (source).
Idea's add extra convolution layers \& use pre-trained model.

Elaborate 16 frame time aspect.

\subsection{Video Vision Transformer}

For training the video vision transformer, the data is upsampeled

\subsection{Multiscale Video Vision Transformer - MViT}
\href{https://pytorch.org/vision/main/models/video_mvit.html}{Pytorch MViT}

Pytorch implementation of source.

For the first training round a double upgrade has been performed. The first adaption is downsampling the skills in 5 main classes to predict.  \& pre-trained model.

\begin{table}[h!]
    \centering
    \begin{tabular}{|r|r|l|r|r|}
        \hline
        \textbf{Train \%} & \textbf{Train Count} & \textbf{Skill} & \textbf{Val Count} & \textbf{Val \%} \\
        \hline
        50.6687 & 2273 & jump & 379 & 56.6517 \\
        14.0660 & 631 & pushup & 77 & 11.5097 \\
        13.0629 & 586 & return from power & 84 & 12.5561 \\
        9.8529 & 442 & frog & 59 & 8.8191 \\
        3.2100 & 144 & crab & 9 & 1.3453 \\
        2.0062 & 90 & split & 12 & 1.7937 \\
        1.1146 & 50 & flip & 5 & 0.7474 \\
        0.9140 & 41 & rondat & 5 & 0.7474 \\
        0.8917 & 40 & rad & 3 & 0.4484 \\
        0.8025 & 36 & suicide & 8 & 1.1958 \\
        0.7356 & 33 & handspring & 5 & 0.7474 \\
        0.6910 & 31 & rol2kip & 7 & 1.0463 \\
        0.4458 & 20 & kip & 6 & 0.8969 \\
        0.3567 & 16 & speed & NULL & NULL \\
        0.3121 & 14 & kopkip & 1 & 0.1495 \\
        0.2675 & 12 & roll & 3 & 0.4484 \\
        0.1783 & 8 & stut & 2 & 0.2990 \\
        0.1337 & 6 & swift & NULL & NULL \\
        0.1337 & 6 & UNKOWN & 1 & 0.1495 \\
        0.0669 & 3 & leapfrog & NULL & NULL \\
        0.0446 & 2 & footwork-kick & NULL & NULL \\
        0.0223 & 1 & mountainclimber & NULL & NULL \\
        0.0223 & 1 & footwork-open & 1 & 0.1495 \\
        \hline
    \end{tabular}
    \caption{Skill distribution with training and validation counts and percentages. Null values indicate missing validation data.}
    \label{tab:skill_distribution_full_with_nulls}
\end{table}


\begin{table}[h!]
    \centering
    \begin{tabular}{|l|r|r|r|r|}
        \hline
        \textbf{Skill} & \textbf{Train Count} & \textbf{Train \%} & \textbf{Val Count} & \textbf{Val \%} \\
        \hline
        jump & 2273 & 50.6687 & 379 & 56.6517 \\
        pushup & 631 & 14.0660 & 77 & 11.5097 \\
        return from power & 586 & 13.0629 & 84 & 12.5561 \\
        frog & 442 & 9.8529 & 59 & 8.8191 \\
        other & 526 & 11.7253 & 68 & 10.1645 \\
        \hline
    \end{tabular}
    \caption{Train and validation skill distribution with low-frequency skills grouped as "other"}
    \label{tab:skill_distribution_grouped_final}
\end{table}


\section{Model verification}

Comming soon... (Mapping predicted skills, turners, rotations... to its perspective level which will be compared with the score assigned by judges on competition.)