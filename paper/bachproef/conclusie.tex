%%=============================================================================
%% Conclusie
%%=============================================================================

\chapter{Discussion}%
\label{ch:discussion}

% TODO: Trek een duidelijke conclusie, in de vorm van een antwoord op de
% onderzoeksvra(a)g(en). Wat was jouw bijdrage aan het onderzoeksdomein en
% hoe biedt dit meerwaarde aan het vakgebied/doelgroep?
% Reflecteer kritisch over het resultaat. In Engelse teksten wordt deze sectie
% ``Discussion'' genoemd. Had je deze uitkomst verwacht? Zijn er zaken die nog
% niet duidelijk zijn?
% Heeft het onderzoek geleid tot nieuwe vragen die uitnodigen tot verder
%onderzoek?

The goal of this research was to increase in the objectivity and accuracy of scores assigned by judges on jump rope freestyles during competitions using recent advancements in machine learning technology.
In order to achieve this, two sets of questions needed answers.
The first set being questions about jump rope, which will be answered in \ref{ch:discussion-jump-rope-answers}. The second set contains questions about the machine learning part, which follows in \ref{ch:machine-learning-answers}. Some of them required an answer before starting the development of the proof of concept, while others depended on the PoC.

\section{Jump rope answers}
\label{ch:discussion-jump-rope-answers}

Today, the challenge for jump rope judges \ref{intro-bp:question-challenges-for-judges} is the ability to keep up with the skills performered, while calculating the skill level, in order to accurately score the difficulty of a routine. In order to reduce errors, judging methods change, such as adapting the rules \ref{lit:adapting-the-rules} or reviewing the routine post-performence at slower speeds \ref{lit:review-at-slower-speed}.

Skills were then broken down \ref{lit:jump-rope-skills-introduction} to better understand how difficulty is scored \ref{intro-bp:question-difficulty-scored}. These were further specified and organized into a skill matrix \ref{lit:skill-matrix} in order to specify which exact actions needed to be recognized \ref{intro-bp:question-what-are-the-skills-to-be-recognized}.
Having skills specified, leads to the machine learning part, transforming this matrix into computer output and exploring machine learning possibilities. % possibilities / results?


\section{Machine learning answers}
\label{ch:machine-learning-answers}

% TODO add sensoric data if time?
Computer vision \ref{lit:computer-vision}, a subset of machine learning, is the ability of computers to understand visual data.
Exploring this topic; models, other sports, the jury support system of Fujitsu (\ref{intro-bp:question-earlier-research-guidance}, \ref{lit:computer-vision-sports}), the availability of video recordings (\ref{intro-bp:question-data}, \ref{tbl:data-comparison-sr-dd}), along with aquiring information about and NextJumps' speedcounter \ref{lit:nextjump-speedcounter} allowed for the choice to learn out of video material \ref{intro-bp:question-which-modern-technologies}.

Using videodata and recognizing actions required to think about quite some properties. Recordings are typically full routines of about 60 to 75 seconds. These freestyles contain 40 to 60 skills, around 100 if you include normal jumps. Some recordings are zoomed-in, others are stationary capturing the whole field. The videotype may be mp4, blu-ray (m2ts) or AVI. Even the framerate could differ (25, 30, 50).
Considering these aspects, a general approach for recognizing skills in a video is created while exploring human action recogniton \ref{lit:human-action-recognition}. This resulted in three steps; jumper localization \ref{lit:jumper-localization}, action segmentation \ref{lit:video-action-segmentation} and skill recognition \ref{lit:skill-recognition}, each of them requiring future work \ref{discussion:future-work}.

% TODO : add refs to methodology or results?

\subsection{Localization}

Localization serves as a way to zoom in and focussing on the actions, sparing computational resources, rather than providing frames to a model where the actions and athletes only take up a limitted amount of space in the video \ref{fig:sr4-field}.

on the  Tests on results for localizing \ref{results:jumper-localization} are limited in order to have spent sufficient time for segmenting and recognizing skills. Even then, time was limited. Predicting the location of jumpers has been tested using full boxes as a first stage. Models in this stage included MobileNet and GoogleNet, but results were lacking. The switch to annotate individuals instead of full teams and using a pre-trained YOLO model quickly reached better results.
Even though the mAP50 reached 0.943 accuracy, intial full videocrops weren't stable, requiring smoothing techniques and more labels in order to reduce spectator predictions and visually disturbing shocks (table \ref{tbl:crop-results}). Utilizing a basic smoothing technique \ref{results:crop-stability} allowed for sufficient videocrops, which can be used for skill recognition \& segmentation.

\subsection{Future work on localization}

Localization can be improved in multiple areas. This could involve training on full team labels using YOLO, applying other smoothing techniques or implenting other models. Other factors could be changing the competition setting to limit the amount of spectators, keeping the camera perspective consistent or provide more different perspectives. Another way is annotating the predicted spectators or judges, indicated by pre-trained models, which can be filtered out when cropping.

\subsection{Segmentation}

The part about action segmentation definitely needs more research and experiments. The easiest implementation was utilizing vision models used for recognizing actions to annotate interesting split points having a value of 1, a split moment, or 0, executing a skill or doing nothing. Using the Multiscale Vision Transformer, developed by \autocite{Fan2021}, which acquired great results recognition \ref{results:skill-recognition}, quickly showed overlap between predicted split points and labeled split points \ref{fig:segmentation-plot}.
The MViT applied for segmentation transforms the annotated skill labels \ref{methodology:skill-recognition} into split labels \ref{methodology:action-segmentation}. A little summary of the labels follows. Skills labels have a start and end frame, which allows to indicate split values at these frame numbers to be around one. An example would be:

\(
[0, 0, 0.1, 0.31, 0.55, 0.86, 1, 0.86, 0.55, 0.31, 0.1, 0, 0, 0 \dots]
\)

Videos were then split into partitions of T = 16 timesteps, learning some context of previous and following frames, in order to predict the corresponding T split output values as wel. The highest peaks were then considered split points \ref{fig:segmentation-plot}.
Being able to predict splitpoints allows to perform skill recognition on completely unseen videos, isolating each action.

\subsection{Future work on action segmentation}

Due to time constraints and priorities, only the MSE has been used to decide the most optimal split moments on the MViT model. There are still future actions which could improve action segmentation. Ideas, not limited to the list below, include:

\begin{itemize}
    \item Implement other metrics for segmentation next to MSE
        \begin{itemize}
            \item IoU section overlap
            \item Average distance to closest split point
            \item Count of predicted splitpoints vs actual splitpoints
        \end{itemize}
    \item Implement a second dimension, is jumping parameter to filter out mistake recoveries or non jumping moments.
    \item Predict every other frame, instead of all frames.
    \item Implementing a action segmentation specific models.
    \item More labels
\end{itemize}

\subsection{Recognition}

Being able to zoom in on athletes and having a model which can segmenting actions, actual skills can be predicted on full videos.
But even before the ability to isolate skills, labeling and training experiments on skill recognition are possible. One minor issue, solved in the methodology of skill recognition \ref{methodology:skill-recognition} was the fact that performed actions have a different length \ref{fig:skilllengths-counts}. This is solved by ensuring they have the same length of T = 16 timesteps, skipping or duplicating frames \ref{methodology:skill-recognition}. This resulted in equal inputs with a dimension of (channels, timesteps, height, width), (C, T, H, W) for short.

Next up was transforming all aspects of skills into machine understandable output labels. This was the reason why the skill matrix has been created \ref{lit:skill-matrix}. The matrix resulted in a skill configuration \ref{code:confighelper} for skill labels, specifying 13 different outputs; Type, Rotations, Turner1, Turner2, Skill, Hands, Feet, Turntable, BodyRotations, Backwards, Sloppy, Hard2see and Fault. Each aspect being a category, numeric value or a boolean.

The final adaptation using weighted losses, which returned a different loss depending on the occurrence of skill aspects pushed the MViT model into grasping the meaning behind the fault, sloppy and hard2see aspects. It raised the f1 average macro accuracy from 45.61\% to 51.53\%, giving in on f1 macro skill accuracy. It fell from 34.93\% to 27.75\%, which is low compared to the skill recognition accuracies on MViT extra dense 31.46\%, SwinT t 32.06\%, and SwinT s 36.73\%. However, it still reached highest on the normal accuracy measure 93.83\%, compared to 93.73\%, 91.99\%, or 93.3\%.

It is unknown how much data is expected to increase the accuracy \ref{intro-bp:question-expected-data-to-increase-accuracy}, but one major improvement is adding sufficient examples of each skill or turner, in order to provide enough feedback to model to learn more about these less represented skill aspects.

\subsection{Future work on skill recognition}

Aside from additional labels, some possibilities to increase accuracy are fine-tuning the weighted losses or experimenting using different models. Another experiment could be predicting the actions of individual athletes, instead of regarding the video section as a single unit. This can be especially usefull when predicting single rope team freestyles, in which the jumpers act as a unit, but also act as an individual, contrary to double dutch freestyles. Currently the data has been downsampled, filtering mostly normal jumps. This downsampling can be adjusted, as it there could be turner skills when there is a normal jump, limiting the training of these turners. Further data augmentation, besides mirroring the visual, such as color shifts, random crops or camera motion can also increase the accuracy of the models. A more difficult research topic, could be to create a full 3D pose of the athletes, which might require multiple cameras. 

\subsection{Judge score}

In the final run, MViT reached the best judge score results, with a difference of -21.68\% compared to the scores allocated by judges.
Time constraints didn't allow for a comparison of judge scores and model predictions with the ground truth. Thus the desired outcome to have a score difference around 5 to 10\% couldn't be measured. Even the -21.68\% is way to far off in order to use at competitions.

\section{Other future work}
\label{discussion:future-work}

When ground truths are available for the routines, an actual comparison between judge scores and models can be made to decide the better judge. Meanwhile, skill matrices for other events can be created, labeled on videos and trained, allowing for a broader range of models, model outputs and possibilities.

To conclude, this research showed the possibilities of the proposed three step architecture for recognizing skills in a full routine on a limited dataset. While results aren't perfect, great effort has been put in the proof of concept, enabling future research. The main priority should now be recording and annotating more videos, with different camera perspectives, including more skill of each skill variation in both the training and validation dataset.
