{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BYbX10klVKl3"
      },
      "outputs": [],
      "source": [
        "# Implement GoogleNet\n",
        "# Build an inception module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8IbB8yAdVX4T"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-01-06 16:15:35.739683: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-01-06 16:15:35.750916: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1736176535.764145  409118 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1736176535.768106  409118 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-06 16:15:35.782085: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import functools\n",
        "import keras\n",
        "\n",
        "DefaultConv = functools.partial(\n",
        "    keras.layers.Conv2D, kernel_size=(1, 1), strides=(1, 1),\n",
        "    padding=\"same\", activation=\"relu\")\n",
        "\n",
        "class InceptionModule(keras.layers.Layer):\n",
        "  def __init__(self, filters11, filters33_reduce, filters33,\n",
        "               filters55_reduce, filters55, filters_pool_proj,\n",
        "               use_batch_norm=True,**kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.conv11 = DefaultConv(filters=filters11)\n",
        "\n",
        "    self.conv33_reduce = DefaultConv(filters=filters33_reduce)\n",
        "    self.conv33 = DefaultConv(filters=filters33, kernel_size=(3,3))\n",
        "\n",
        "    self.conv55_reduce = DefaultConv(filters=filters55_reduce)\n",
        "    self.conv55 = DefaultConv(filters=filters55, kernel_size=(5,5))\n",
        "\n",
        "    self.max_pool33 = keras.layers.MaxPool2D(pool_size=(3,3), strides=(1,1), padding='same')\n",
        "    self.conv_pool = DefaultConv(filters=filters_pool_proj)\n",
        "\n",
        "    self.use_batch_norm = use_batch_norm\n",
        "    if use_batch_norm:\n",
        "      self.batch_layer = keras.layers.BatchNormalization()\n",
        "\n",
        "  def call(self, inputs):\n",
        "    path1 = self.conv11(inputs)\n",
        "    path2 = self.conv33(self.conv33_reduce(inputs))\n",
        "    path3 = self.conv55(self.conv55_reduce(inputs))\n",
        "    path4 = self.conv_pool(self.max_pool33(inputs))\n",
        "    # print('path1', path1.shape)\n",
        "    # print('path2', path2.shape)\n",
        "    # print('path3', path3.shape)\n",
        "    # print('path4', path4.shape)\n",
        "\n",
        "    iModule = keras.layers.Concatenate(axis=-1)([path1, path2, path3, path4])\n",
        "    # print('imodule', iModule.shape)\n",
        "    logits = self.batch_layer(iModule) if self.use_batch_norm else iModule\n",
        "\n",
        "    return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vimj5j-qcVLY"
      },
      "outputs": [],
      "source": [
        "DefaultMaxPool = functools.partial(\n",
        "    keras.layers.MaxPool2D,\n",
        "    pool_size=(3,3), strides=(2,2), padding=\"same\")\n",
        "\n",
        "def get_googlenet_model(input_shape, num_classes, use_batch_norm=True, **kwargs):\n",
        "  model = keras.Sequential(**kwargs)\n",
        "  model.add(keras.layers.Input(shape=input_shape))\n",
        "  model.add(DefaultConv(filters=64, kernel_size=(7,7), strides=(2,2),  padding='same'))\n",
        "  if use_batch_norm:\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "  model.add(DefaultMaxPool())\n",
        "  model.add(DefaultConv(filters=64))\n",
        "  model.add(DefaultConv(filters=192, kernel_size=(3,3)))\n",
        "  if use_batch_norm:\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "  model.add(DefaultMaxPool())\n",
        "\n",
        "  model.add(InceptionModule(filters11=64, filters33_reduce=96, filters33=128,\n",
        "    filters55_reduce=16, filters55=32, filters_pool_proj=32,\n",
        "    use_batch_norm=use_batch_norm))\n",
        "  model.add(InceptionModule(filters11=128, filters33_reduce=128, filters33=192,\n",
        "    filters55_reduce=32, filters55=96, filters_pool_proj=64,\n",
        "    use_batch_norm=use_batch_norm))\n",
        "  model.add(DefaultMaxPool())\n",
        "\n",
        "  model.add(InceptionModule(filters11=192, filters33_reduce=96, filters33=208,\n",
        "    filters55_reduce=16, filters55=48, filters_pool_proj=64,\n",
        "    use_batch_norm=use_batch_norm))\n",
        "  model.add(InceptionModule(filters11=160, filters33_reduce=112, filters33=224,\n",
        "    filters55_reduce=24, filters55=64, filters_pool_proj=64,\n",
        "    use_batch_norm=use_batch_norm))\n",
        "  model.add(InceptionModule(filters11=128, filters33_reduce=128, filters33=256,\n",
        "    filters55_reduce=24, filters55=64, filters_pool_proj=64,\n",
        "    use_batch_norm=use_batch_norm))\n",
        "  model.add(InceptionModule(filters11=112, filters33_reduce=144, filters33=288,\n",
        "    filters55_reduce=32, filters55=64, filters_pool_proj=64,\n",
        "    use_batch_norm=use_batch_norm))\n",
        "  model.add(InceptionModule(filters11=256, filters33_reduce=160, filters33=320,\n",
        "    filters55_reduce=32, filters55=128, filters_pool_proj=128,\n",
        "    use_batch_norm=use_batch_norm))\n",
        "\n",
        "  model.add(DefaultMaxPool())\n",
        "  model.add(InceptionModule(filters11=256, filters33_reduce=160, filters33=320,\n",
        "    filters55_reduce=32, filters55=128, filters_pool_proj=128,\n",
        "    use_batch_norm=use_batch_norm))\n",
        "  model.add(InceptionModule(filters11=384, filters33_reduce=192, filters33=384,\n",
        "    filters55_reduce=48, filters55=128, filters_pool_proj=128,\n",
        "    use_batch_norm=use_batch_norm))\n",
        "  model.add(keras.layers.GlobalAveragePooling2D())\n",
        "  model.add(keras.layers.Dropout(0.4))\n",
        "  model.add(keras.layers.Flatten())\n",
        "  model.add(keras.layers.Dense(units=1000, activation=\"relu\"))\n",
        "  model.add(keras.layers.Dense(units=4, activation='sigmoid'))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jYkmSEyCjW9-",
        "outputId": "66db7051-d043-44f7-ec39-2585721a9b07"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">110,784</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ inception_module_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">164,720</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InceptionModule</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ inception_module_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">390,656</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InceptionModule</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ inception_module_3              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">378,224</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InceptionModule</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ inception_module_4              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">451,208</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InceptionModule</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ inception_module_5              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">512,152</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InceptionModule</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ inception_module_6              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">607,488</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InceptionModule</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ inception_module_7              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">871,680</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InceptionModule</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ inception_module_8              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,046,784</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InceptionModule</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ inception_module_9              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,448,176</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InceptionModule</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,004</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m9,472\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m4,160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m192\u001b[0m)  │       \u001b[38;5;34m110,784\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m192\u001b[0m)  │           \u001b[38;5;34m768\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m192\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ inception_module_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │       \u001b[38;5;34m164,720\u001b[0m │\n",
              "│ (\u001b[38;5;33mInceptionModule\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ inception_module_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m480\u001b[0m)  │       \u001b[38;5;34m390,656\u001b[0m │\n",
              "│ (\u001b[38;5;33mInceptionModule\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m480\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ inception_module_3              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │       \u001b[38;5;34m378,224\u001b[0m │\n",
              "│ (\u001b[38;5;33mInceptionModule\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ inception_module_4              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │       \u001b[38;5;34m451,208\u001b[0m │\n",
              "│ (\u001b[38;5;33mInceptionModule\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ inception_module_5              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │       \u001b[38;5;34m512,152\u001b[0m │\n",
              "│ (\u001b[38;5;33mInceptionModule\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ inception_module_6              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m528\u001b[0m)    │       \u001b[38;5;34m607,488\u001b[0m │\n",
              "│ (\u001b[38;5;33mInceptionModule\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ inception_module_7              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m832\u001b[0m)    │       \u001b[38;5;34m871,680\u001b[0m │\n",
              "│ (\u001b[38;5;33mInceptionModule\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_11 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m832\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ inception_module_8              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m832\u001b[0m)    │     \u001b[38;5;34m1,046,784\u001b[0m │\n",
              "│ (\u001b[38;5;33mInceptionModule\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ inception_module_9              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1024\u001b[0m)   │     \u001b[38;5;34m1,448,176\u001b[0m │\n",
              "│ (\u001b[38;5;33mInceptionModule\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │     \u001b[38;5;34m1,025,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │         \u001b[38;5;34m4,004\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,025,532</span> (26.80 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,025,532\u001b[0m (26.80 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,014,044</span> (26.76 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,014,044\u001b[0m (26.76 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,488</span> (44.88 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m11,488\u001b[0m (44.88 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = get_googlenet_model(input_shape=(1024,1024,3), num_classes=4)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p6yFRBi05lbD"
      },
      "outputs": [],
      "source": [
        "def iou(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate IoU loss between the true and predicted bounding boxes.\n",
        "\n",
        "    y_true and y_pred should have the shape (batch_size, 4), where each element is\n",
        "    [center_x, center_y, width, height].\n",
        "    \"\"\"\n",
        "    # Convert (center_x, center_y, width, height) to (xmin, ymin, xmax, ymax)\n",
        "    true_xmin = y_true[..., 0] - 0.5 * y_true[..., 2]\n",
        "    true_ymin = y_true[..., 1] - 0.5 * y_true[..., 3]\n",
        "    true_xmax = y_true[..., 0] + 0.5 * y_true[..., 2]\n",
        "    true_ymax = y_true[..., 1] + 0.5 * y_true[..., 3]\n",
        "\n",
        "    pred_xmin = y_pred[..., 0] - 0.5 * y_pred[..., 2]\n",
        "    pred_ymin = y_pred[..., 1] - 0.5 * y_pred[..., 3]\n",
        "    pred_xmax = y_pred[..., 0] + 0.5 * y_pred[..., 2]\n",
        "    pred_ymax = y_pred[..., 1] + 0.5 * y_pred[..., 3]\n",
        "\n",
        "    # Calculate the intersection area\n",
        "    inter_xmin = keras.ops.maximum(true_xmin, pred_xmin)\n",
        "    inter_ymin = keras.ops.maximum(true_ymin, pred_ymin)\n",
        "    inter_xmax = keras.ops.minimum(true_xmax, pred_xmax)\n",
        "    inter_ymax = keras.ops.minimum(true_ymax, pred_ymax)\n",
        "\n",
        "    inter_width = keras.ops.maximum(0.0, inter_xmax - inter_xmin)\n",
        "    inter_height = keras.ops.maximum(0.0, inter_ymax - inter_ymin)\n",
        "    intersection_area = inter_width * inter_height\n",
        "\n",
        "    # Calculate the union area\n",
        "    true_area = (true_xmax - true_xmin) * (true_ymax - true_ymin)\n",
        "    pred_area = (pred_xmax - pred_xmin) * (pred_ymax - pred_ymin)\n",
        "    union_area = true_area + pred_area - intersection_area\n",
        "\n",
        "    # Calculate IoU\n",
        "    iou = intersection_area / union_area\n",
        "\n",
        "    return iou\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qrwAIHKh52kp"
      },
      "outputs": [],
      "source": [
        "# Compile the model with IoU loss\n",
        "# model.compile(optimizer='adam', loss=keras.losses.Huber(), metrics=[iou_loss])\n",
        "model.compile(optimizer='adam', loss='mse', metrics=[iou])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdxVTJuxJVds"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "connection established\n",
            "connection established\n",
            "DataGeneratorSkillBorders init done\n",
            "on_epoch_end_called\n",
            "connection established\n",
            "DataGeneratorSkillBorders init done\n",
            "on_epoch_end_called\n",
            "on_epoch_end_called\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1736176575.196860  409233 service.cc:148] XLA service 0x7b61900045e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1736176575.196885  409233 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9\n",
            "2025-01-06 16:16:15.587063: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2025-01-06 16:16:19.573937: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7413', 236 bytes spill stores, 236 bytes spill loads\n",
            "\n",
            "2025-01-06 16:16:20.716989: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.41GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "E0000 00:00:1736176581.654051  409233 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0000 00:00:1736176581.817790  409233 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "E0000 00:00:1736176581.983633  409233 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
            "2025-01-06 16:16:24.592275: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.65GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2025-01-06 16:16:27.025368: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[32,192,196,196]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,64,196,196]{3,2,1,0}, f32[192,64,3,3]{3,2,1,0}, f32[192]{0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
            "2025-01-06 16:16:28.117835: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 2.092565883s\n",
            "Trying algorithm eng0{} for conv (f32[32,192,196,196]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,64,196,196]{3,2,1,0}, f32[192,64,3,3]{3,2,1,0}, f32[192]{0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
            "2025-01-06 16:16:28.117900: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.07GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2025-01-06 16:16:34.653472: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[32,192,98,98]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,98,98]{3,2,1,0}, f32[192,128,3,3]{3,2,1,0}, f32[192]{0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
            "2025-01-06 16:16:34.682008: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.028641846s\n",
            "Trying algorithm eng0{} for conv (f32[32,192,98,98]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,98,98]{3,2,1,0}, f32[192,128,3,3]{3,2,1,0}, f32[192]{0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
            "2025-01-06 16:16:34.682070: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.57GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2025-01-06 16:16:52.223360: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.53GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2025-01-06 16:17:02.458887: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.11GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2025-01-06 16:17:09.897498: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[32,128,98,98]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,192,98,98]{3,2,1,0}, f32[192,128,3,3]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
            "2025-01-06 16:17:09.922160: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.024725863s\n",
            "Trying algorithm eng0{} for conv (f32[32,128,98,98]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,192,98,98]{3,2,1,0}, f32[192,128,3,3]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
            "2025-01-06 16:17:18.360694: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.19GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2025-01-06 16:17:21.232761: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[32,64,196,196]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,192,196,196]{3,2,1,0}, f32[192,64,3,3]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
            "2025-01-06 16:17:22.635314: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 2.402644681s\n",
            "Trying algorithm eng0{} for conv (f32[32,64,196,196]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,192,196,196]{3,2,1,0}, f32[192,64,3,3]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
            "2025-01-06 16:17:28.059627: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[64,3,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,3,785,785]{3,2,1,0}, f32[32,64,392,392]{3,2,1,0}), window={size=7x7 stride=2x2 pad=2_2x2_2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
            "2025-01-06 16:17:28.622807: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.563263664s\n",
            "Trying algorithm eng0{} for conv (f32[64,3,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,3,785,785]{3,2,1,0}, f32[32,64,392,392]{3,2,1,0}), window={size=7x7 stride=2x2 pad=2_2x2_2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
            "2025-01-06 16:17:32.885290: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.94GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2025-01-06 16:17:38.802965: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.28GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2025-01-06 16:17:55.680168: W external/local_xla/xla/service/hlo_rematerialization.cc:3005] Can't reduce memory use below 3.88GiB (4172059063 bytes) by rematerialization; only reduced to 7.25GiB (7790665944 bytes), down from 9.04GiB (9706800520 bytes) originally\n"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from FrameLoader import FrameLoader\n",
        "from DataGeneratorFrames import DataGeneratorFrames\n",
        "from DataRepository import DataRepository\n",
        "\n",
        "repo = DataRepository()\n",
        "repo.load_relativePaths_of_videos_with_framelabels()\n",
        "\n",
        "DIM = 320\n",
        "train_generator = DataGeneratorFrames(\n",
        "    frameloader=FrameLoader(repo),\n",
        "    train_test_val=\"train\",\n",
        "    dim=(DIM,DIM),\n",
        "    batch_size=32,\n",
        ")\n",
        "\n",
        "val_generator = DataGeneratorFrames(\n",
        "    frameloader=FrameLoader(repo),\n",
        "    train_test_val=\"test\",\n",
        "    dim=(DIM,DIM),\n",
        "    batch_size=32,\n",
        ")\n",
        "\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint('model_best.keras', save_best_only=True, monitor='loss', mode='min', verbose=1),\n",
        "    EarlyStopping(monitor='loss', patience=10, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=1)\n",
        "]\n",
        "\n",
        "# Fit the model using the data generator and callbacks\n",
        "history = model.fit(\n",
        "    train_generator,  # Training data generator\n",
        "    epochs=5,        # Set the number of epochs\n",
        "    callbacks=callbacks,  # List of callbacks\n",
        "    verbose=1,         # Verbose output\n",
        "    validation_data=val_generator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hA57-G3yJwPZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((0,), (0,))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X, y = train_generator.__getitem__(5)\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
